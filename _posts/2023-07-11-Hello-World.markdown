---
layout: post
title: "Motivating Gradient Descent"
date: 2023-07-11
---

Gradient descent is the main optimizer used to solve for the objective function in many machine learning algorithms. In this post,
I will motivate a family of iterative optimization algorithms and we will see that gradient descent is a just particular case or base case under this framework.

Ww will start with the following lemma:

Let $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$ be continously differentiable in an open region $D \cup \mathbb{R}^{n}$. For all $x \in D$ and $p \in \mathbb{R}^{n}$,
the directional derivative in the $p$ direction, defined by 

$$
ddd
$$

